---
title: "Exercise Week 4 (limma fundamentals)"
author: "Elina Jansone"
format:
  html:
    self-contained: true
editor: source
editor_options: 
  chunk_output_type: console
---

## Import libraries

```{r}
library("limma")
library("ggplot2")
install.packages(c('matrixstats','tidyr','ggallin'))
```

## Initial input values

```{r}
nGenes <- 10000                   # number of "features"
nSamples <- 6                     # number of samples (split equal in 2 groups)
pDiff <- .1                       # percent of genes "differential 
grp <- rep(0:1,each=nSamples/2)   # dummy variable for exp. group
trueFC <- 2                     # log-fold-change of truly DE

d0 <- 5
s0 <- 0.8
sd <- s0*sqrt(d0/rchisq(nGenes,df=d0))  # dist'n of s.d.
```

Note: there are some details regarding the scaled inverse chi-square distribution that you may want to explore. For example, see the [wiki description](https://en.wikipedia.org/wiki/Scaled_inverse_chi-squared_distribution).

### Generate a table of (null) data (i.e., no differential features):

```{r}
y <- matrix(rnorm(nGenes*nSamples,sd=sd),
            nr=nGenes,nc=nSamples)
```

And, we can add in "differential expression", randomly chosen to be in the positive or negative direction, to a set of indices chosen:

```{r}
indD <- 1:floor(pDiff*nGenes)
diff <- sample(c(-1,1),max(indD),replace=TRUE)*trueFC
y[indD,grp==1] <- y[indD,grp==1] + diff
```

## Question 1

Exploratory analysis of the true and observed variances:

For the observed variances, 

* compute the residual variance for each row of y (i.e., pooled variance of the two simulated samples, not the row-wise variance; see the denominator of the classical two-sample t-statistic) 

* look at the distribution of them, of the true variances (from the simulated sd) and a scatter plot of true versus observed. 

Sometimes viewing variances on the log scale is preferred.

[Multiple t-testing](https://statsandr.com/blog/student-s-t-test-in-r-and-by-hand-how-to-compare-two-groups-under-different-scenarios/)

```{r}
library(matrixStats)

g1 <- y[,1:(nSamples %/% 2)]
g2 <- y[,(nSamples %/% 2 + 1):(nSamples)]

var1 = rowVars(g1)
var2 = rowVars(g2)
```

### Calculate pooled and true variance:

```{r}
pooled_var = sqrt( 
  ((nSamples %/% 2 - 1)*var1^2 + 
    (nSamples %/% 2 - 1)*var2^2) / 
    (nSamples %/% 2*2 - 2) )

pooled_true_var_df <- data.frame(cbind(pooled_var, sd^2))

colnames(pooled_true_var_df) <- c('Observed', 'True')

```

### Plotting ooled and true variance:

```{r}
pooled_var_hist = hist(pooled_var) # needs log10, log1p not enough
pooled_log_var_hist = hist(log10(pooled_var)) 

true_var_hist = hist(log10(sd^2)) # slightly skewed still

library(tidyr)

variances_df_long <- pooled_true_var_df %>%
  pivot_longer(c(1,2),names_to = 'Type', values_to = 'Variance')

# plotting log10 of obs. and true variance 
# => compare distribution

ggplot(data = data.frame(variances_df_long),
       aes(x= c(1:length(rownames(variances_df_long))),          y = log10(Variance))) + 
  geom_point() +
  facet_wrap(~Type)

```

## Question 2

Produce an additional visualization to show that you understand the differential expression that we introduced into the simulation.

### Volcano plot for visualizing differential gene expression
```{r}
sample_data <- y

# Calculate p values for differential expression:

t_test_multiple <- function(df, g1, g2) {
  x = df[g1]
  y = df[g2]
  x = as.numeric(x)
  y = as.numeric(y)  
  results = t.test(x, y)
  results$p.value
}

pvalues_raw <- apply(sample_data, 1, t_test_multiple, 
                   g1 = c(1:nSamples %/% 2), 
                   g2 = c((nSamples %/% 2 + 1):(nSamples)))

means_g1 <- c(1:nGenes)
means_g2 <- c(1:nGenes)

for (i in c(1:nGenes)) {
  means_g1[i] <- mean(sample_data[i,1:nSamples %/% 2])
  means_g2[i] <- mean(sample_data[i,(nSamples %/% 2 + 1):(nSamples)])
}
L2FC <- means_g2 - means_g1

diff_df <- data.frame(cbind(pvalues_raw, L2FC))

diff_df$gene_no <- rownames(diff_df)

ggplot(data=diff_df, 
       aes(x=L2FC, y=-log10(pvalues_raw),
           label = gene_no)) +
  geom_point() +
  geom_hline(yintercept = 4, color = 'red') +
  geom_vline(xintercept = -2, color = 'red') + 
  geom_vline(xintercept = 2, color = 'red') +
  geom_text(aes(label=ifelse(-log10(pvalues_raw)>4 & abs(L2FC) > 2,gene_no,'')),
            hjust=0,vjust=0)
```

## Question 3

Create a design matrix to represent the linear model fit (to each row of the table) by limma:

```{r}
design <- model.matrix(~grp)
```

### In terms of the model that is fit for each feature, what is the interpretation of the two columns of this design matrix?

Groups are 0 and 1, probably just to distinguish the two. Maybe 0 is the reference. The lm() function can use factors where one is usually kept as a reference... if I follow the lm() logic then the null hypothesis intercept is given for each sample. Maybe the function also sets the intercept values as constants so that only grp effect is quantified. Kinda like setting it up for ANOVA?

```{r}
fit <- lmFit(y,design)
fit <- eBayes(fit)
names(fit)
topTable(fit)
```

First, the linear model is fit for each feature. Second, the variance parameters are moderated and then moderated t/F statistics can be calculated. Several elements are added to the fit object after these two steps; you may wish to consult the help page ?"MArrayLM-class" for further details these elements.

Below, a vector of colours is used to signify the true differential "status", which will be used in exploratory plots (below, the moderated t-statistic).

```{r}
cols <- rep("non-differential",nrow(y))
cols[indD] <- "differential"

qplot(y=fit$t[,2]) + geom_point(aes(colour=cols)) 

```

## Question 4

For each row of y, calculate also the classical 2-sample t-test. See ?t.test for more details about the built-in R function to do this calculation and convince yourself which arguments to use to match the classical t-test described in the lecture. Add a visualization similar to the above plot for the classical t-statistic and the log-fold-change (mean difference of the 2 groups). Which statistic best separates the truly differential from non-differential?

Classical t-statistics

```{r}
df <- y 
ttest_example <- function(df, grp1, grp2) {
  x = df[grp1]
  y = df[grp2]
  x = as.numeric(x)
  y = as.numeric(y)  
  results = t.test(x, y, alternative = "two.sided" )
  results$statistic
}
t_statistic_classic = apply(df, 1, ttest_example, grp1 = c(1:3), grp2 = c(4:6))
t_statistics_moderate = fit$t[,2]

qplot(y=t_statistic_classic) + geom_point(aes(colour=cols)) 
```

log-fold change

```{r}
df <- y
group1 = apply(df[,1:3], 1, mean)
group2 = apply(df[, 4:6], 1, mean) 
log2foldchange <- group2 / group1

qplot(y=log2foldchange) + 
  geom_point(aes(colour=cols)) +
  scale_y_continuous(trans = pseudolog10_trans)

```

If you look at the space between the red dots from the0 line it looks already quit clear which method seperates the differential expressed genes from the "regular" genes the best, more clearer it get with the ROC curve.
## Question 5

Pick a reasonable metric to compare the methods, such as an ROC curve, false discovery plot, power versus achieved FDR. Using this metric/curve, formally compare the performance of the classical t-test, the moderated t-test and the log-fold-change or mean difference (fit\$coef). Two packages that are useful for these kind of plots include: ROCR or iCOBRA. You can add options to executable code like this

```{r}
df <- y 
ttest_example <- function(df, grp1, grp2) {
  x = df[grp1]
  y = df[grp2]
  x = as.numeric(x)
  y = as.numeric(y)  
  results = t.test(x, y, alternative = "two.sided" )
  #results$p.value
  results$statistic
}
pvalue_classic = apply(df, 1, ttest_example, grp1 = c(1:3), grp2 = c(4:6))
t_value_classic = apply(df, 1, ttest_example, grp1 = c(1:3), grp2 = c(4:6))

pvalue_moderate = (fit$p.value)[,2]

group1 = apply(df[,1:3], 1, mean)
group2 = apply(df[, 4:6], 1, mean) 
foldchange <- group2 / group1

label <- rep(0,nrow(y))
label[indD] <- 1

```

```{r}
library(ROCR)

range01 <- function(x){(x-min(x))/(max(x)-min(x))}

abs_t_statistics_moderate = abs(t_statistics_moderate)
norm_tvalue_mod = range01(abs_t_statistics_moderate)

abs_foldchange = abs(foldchange)
norm_foldchange = range01(abs_foldchange)

abs_classic = abs(t_value_classic)
norm_classic = range01(abs_classic)


pred_classic = prediction(norm_classic, label)
#pred_mod = prediction(pvalue_moderate, label)
pred_mod = prediction(norm_tvalue_mod, label)
pred_fc = prediction(norm_foldchange, label)

perf_fc = performance(pred_fc, "tpr", "fpr")
perf_classic <- performance(pred_classic, "tpr", "fpr")
perf_mod <- performance(pred_mod, "tpr", "fpr")

auc_ROCR_classic <- performance(pred_classic, measure = "auc")
auc_ROCR_classic <- auc_ROCR_classic@y.values[[1]]

auc_ROCR_mod <- performance(pred_mod, measure = "auc")
auc_ROCR_mod <- auc_ROCR_mod@y.values[[1]]

auc_ROCR_fc <- performance(pred_fc, measure = "auc")
auc_ROCR_fc <- auc_ROCR_fc@y.values[[1]]



plot(perf_classic, col="blue")
plot(perf_mod, add = TRUE, col="green")
plot(perf_fc, add = TRUE, col="red") # problem as log fold change can be post or negative
abline(a = 0, b = 1)

# print(paste("ROC AUC (area under curve) of logfoldchange:", auc_ROCR_fc))
# print(paste("ROC AUC (area under curve) of classic ttest:", auc_ROCR_classic))
# print(paste("ROC AUC (area under curve) of moderate ttest:", auc_ROCR_mod))

```
**ROC AUC (area under curve) of logfoldchange:** `r auc_ROCR_fc` <br />
**ROC AUC (area under curve) of classic ttest:** `r auc_ROCR_classic` <br />
**ROC AUC (area under curve) of moderate ttest:** `r auc_ROCR_mod` <br />



Next, we will run a standard 'limma' differential expression (DE) analysis on a real microarray dataset. In particular, we will explore the combination of design matrices and contrast matrices to answer DE questions-of-interest. If you need additional resources to understand this exercise or the methods behind it, it is strongly encourage to read both the limma paper and the limma user's guide; the main details are also given in the lecture.

```{r}
library("affy")
library("preprocessCore")
unzip("affy_estrogen.zip")
ddir <- "affy_estrogen"
dir(ddir)

```

It is generally good practice to store the details of an experiment (e.g., the set of samples) in a machine-readable table, like the provided \`\`targets.txt'' file; this is known as metadata. Have a look at this file in a text editor or a spreadsheet to see what kind of information is typically described. The following code reads in this metadata file, reads in the Affymetrix data and processes it with a method called RMA (robust multichip analysis).

```{r}
# preprocess affymetrix data
targets <- readTargets("targets.txt", path=ddir)
targets$time.h <- factor(targets$time.h)
targets

```

```{r}
abatch <- ReadAffy(filenames=targets$filename,
                   celfile.path=ddir)
eset <- rma(abatch)  # bg correct, normalize, summarize
```

It is also good practice to look at overall summaries of a large dataset, such as a multidimensional scaling (MDS) plot to get an idea of the relations between samples. In this case, "distances on the plot approximate the typical log2 fold changes" (?plotMDS):

```{r}
mds <- plotMDS( exprs(eset), plot = FALSE)  # MDS plot
qplot(x=mds$x, mds$y) + 
  geom_point(aes(shape=targets$estrogen, 
                 colour=targets$time.h), size=4)
```

In order to run the standard limma pipeline for differential expression, we need a design matrix and optionally, a contrast matrix. In the code below, the metadata is encoded into a factor variable that is used for creating the design matrix. It is suggested to look at and understand the design matrix before proceeding.

```{r}
# do the limma modeling
f <- paste(targets$estrogen,targets$time.h,sep="")
f <- factor(f)

# create design matrix
design <- model.matrix(~0+f)
colnames(design) <- levels(f)
design
```

At this stage, it may make sense to filter out control probesets or remove lowly expressed genes (and you will see this in other pipelines), but for simplicity, we go straight to the model fitting. From the design matrix, we can now fit the linear model (for each gene):

```{r}
fit <- lmFit(eset, design)
```

To make inferences about parameters defined in the design matrix, we can now define a contrast matrix, which can be constructed by hand or by using the makeContrasts() function. Again, it is suggested to study this matrix and make sure you understand what it is doing (i.e., in terms of model parameters) before proceeding.

https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7873980/

```{r}
cont.matrix <- makeContrasts(E10="present10-absent10",
                             E48="present48-absent48",
                             Time="absent48-absent10",levels=design)
cont.matrix
```

Now, the contrasts can be fit and the moderation of the variance parameters (as discussed in lectures) can be performed:

```{r}
fit2  <- contrasts.fit(fit, cont.matrix)
fit2  <- eBayes(fit2)
class(fit2)
```

```{r}
names(fit2)
```

At this point, a lot of elements have now been added to the fit2 object and it would again be worth studying the details. See if you can understand what the different components are and if you need more details, type ?"MArrayLM-class" to see more information.

Next, we wish to summarize the differential expression statistics, such as via moderated-t (or F) statistics and perhaps (adjusted) P-values. The topTable() function has many facilities for this:

```{r}
topTable(fit2, coef=1, n=5)

```

```{r}
topTable(fit2, coef=2, n=5)
```

Here, it is worth understanding exactly what coef=1 or coef=2 are testing before proceeding.

It is also recommended to look at the data that goes into the statistical test in the first place.For example, a plot for one gene's expression:

```{r}
qplot(x=f, y=exprs(eset)["39642_at",],) + 
  geom_point(aes(shape=targets$estrogen, 
                 colour=targets$time.h), size=4)

```

## Question 6

From the matrix of summarized Affymetrix data that went into the limma pipeline in the first place (exprs(eset)), manually calculate the logFC and AveExpr for one of the top differentially expressed features

I am a little bit confused. Limma uses a linear model to calculate the log expression values which takes several parameter into account such as inter gene correlation precision weight etc. So if I calculate it manually I will never get the same result unless I also calculat the linear model manually.

```{r}
dataset = exprs(eset)

fit_matrix = topTable(fit2, coef=1, number=dim(dataset)[1])
max_row = fit_matrix[which.max(fit_matrix$logFC),]
int_row = rownames(max_row)

dataset_row = dataset[c(int_row),]
mean_man = mean(dataset_row)

# I only have choosen 1,2 against 3,4 as coef 1 is only present10-absent10.
log_man = log2(mean(dataset_row[1:2])/mean(dataset_row[5:6]))
#log_man = mean(dataset_row[1:2])/mean(dataset_row[5:6])

# print(paste("Average expression level calculated with LIMMA pipeline:",max_row$AveExpr))
# print(paste("Average expression level calculated manually:", mean_man))
# print(paste("LogFC calculated by LIMMA pipeline:", max_row$logFC))
# print(paste("LogFC calculated manually:", as.character(log_man)))
```
Average expression level calculated with LIMMA pipeline: `r  max_row$AveExpr` <br />
Average expression level calculated manually: `r mean_man` <br />
LogFC calculated by LIMMA pipeline: `r max_row$logFC` <br />
LogFC calculated manually: `r log_man` <br />

I am not sure if I did something wrong, the difference looks quit big to me!

Important side note: if your experiment is sufficiently simple (e.g., a 1-way design), you may not need a contrast matrix at all; the differential expression of interest can be defined through column(s) of the design matrix (see discussion in the lecture notes).

## Informations

Note: Submit both the quarto file as well as a compiled HTML file to your private github repository. Note: A convenient way to put all of the contents of a quarto document in a single HTML file is to use the self-contained: true option, as in the header below. Of course, you would need to put this in the header part at the top of your QMD file.

---
title: "Exercise Week 4"
format:
  html:
    self-contained: true
editor: source
editor_options: 
  chunk_output_type: console
---
